# 第一章 大语言模型初识

从2022年末ChatGPT的横空出世，到如今DeepSeek再度进入大众视野，大语言模型已经不断深入各个领域，走进千家万户。大语言模型的迭代可谓层出不穷，最初从 OpenAI 的GPT-2 GPT-3 、Google 的 T5、Meta AI 的 Llama；国内的 GLM、百川智能、零一万物、通义千问、文心一言、书生浦语、智谱清言......可以说，从2023年起，ChatGPT问世之后中国也爆发了“百模大战”。首先我们要了解，语言模型是如何发展到如今的？

## 语言模型的发展历程

在1990年代，那时神经网络 MLP 和 BP 反向传播算法的刚被提出，机器学习在那时仍然主要在 PCA、SVM 这些数学统计方法集中，还未有如今深度学习发展的技术。实际上在那个时代，传统统计机器学习方法就已经开始应用于对语言数据进行统计建模。

语言模型的目标建模本质上就是拟合一个概率分布，假设有一个词汇表 $V$ 。对于一句含有 $m$ 个词语的话，我们可以表示为 $w_1 w_2 \cdots w_m$，而语言模型本质上就是要建模拟合概率分布。

然而，由于现实中语言一句话长度往往很长，对于汉字更是数以万计，如果要想直接通过 对 $w_1w_2\cdots w_m$ 建模，此时语言模型的存储量和参数量就会极其大！

（例如：可以想象生成图片，对于 28 * 28 = 784 的 MNIST 图像大小，图像的值是 Binary 的二进值。要想把每个情况都考虑，那基本上要达到 $2^{784}$ 的规模，这个规模的极其大的！！而语言的长度、汉字词语的组合更是极其复杂！）

### 统计语言模型 （SLM）

我们都知道现在的大语言模型都是使用了自回归方式预测，也就是常见的“Next Token Prediction”，说白了点，就是“**前面所有的，预测下一步**”。本质上也就是

$P(w_1 w_2 \cdots w_m) = \prod_{i=1}^m P(w_i|w_{<i})$

这就是我们通常说的链式法则，但如果基于过去的单词个数较多就会增加计算开销。于是，n-gram 模型基于 Markov 假设。也就是说这里面

$P(w_i | w_{<i}) = P(w_i|w_{i-1-n : i})$

当 $n$ 取1时，就是一个 Markov 过程。

这本质上属于统计机器学习模型，其具备一定的生成能力。那么这个模型是如何去估计的？

- 基于频率的估计：假设有一个词典，里面记录了很多词语以及句子。如果 “我 非常 喜欢” 这段出现了 100 次，下一句 ” 我 非常 喜欢 你“ 出现了 10 次，”我 非常 喜欢 学习“ 出现了20 次，则根据条件概率公式我们有: $P(你 | 我 非常 喜欢) =  P( 我 非常 喜欢 你)  /  P(我 非常 喜欢)=0.1$
- 上面的本质就是统计语言模型的作用，其基于频率去估计条件概率。那在下一次大致就可以达到 如果说 ”我非常喜欢“，则 10% 的概率会说”我非常喜欢你“.....

但这一方法有什么问题呢？

1. 需要有一个词典的要求！如果这个词典太大了，那存储开销和计算、搜索查询过程的开销会变得非常大。
2. 如果出现了一段话不在词典里，例如“我非常喜欢数学“ 这句话，没有出现在这个词典里，那这个概率就是0！而语言组合千变万化很容易出现这个情况！
3. 早期解决的方法之中，一种是增加一个平滑，就是让未出现的一段词语分配一个较小的权重，使得其概率权值不是0。也就是 Laplace Smooth。另一种是回退，本质上就是把单词缩小一点，扩大一些范围。比如“我 非常 喜欢” 假设出现很少，那可以把这个条件改为 ”非常 喜欢“，去掉 "我"，这样，"非常 喜欢 老师讲的...." 就可能出现。通过回退一步，略扩大一些范围。

我们发现，早期的统计语言模型，在没有深度网络时，都需要靠收集统计大量的语言数据。而我们能够收集到的语言数据更是有限，这在浩瀚如海的文字中甚至只不过是 “大海捞针” 的程度。**本质上仍然是不能解决数据稀缺的问题。**

### 神经语言模型 （NLM）

深度神经网络开始引入之后，AI 进入了深度学习时代，神经网络在计算机视觉（CV）领域中，图像分类、目标检测中得到了广泛应用。而在早期自然语言处理（NLP）也发挥了一定作用。

神经网络说到底本质更像一个“拟合函数”。早期利用多层感知机（MLP），也就是把自然语言的单词，映射到一个向量。例如

- 我 --- [0.1, 0.8, 0.9, 0.4, ..]  喜欢 --- [ 0.4, 0.9, 0.5, ....]。
- 通过这种映射方式，把语言从自然语言空间，转换到数字空间，也就是能够被计算机计算，从而达到一个预测的目的

熟悉深度学习的大家都知道 RNN 循环神经网络在早期对带有序列性的数据处理具有一定的优势，其可以通过前序的数据的隐空间特征，结合当前数据的输出预测下一个时间步长的数据。RNN 早起就被应用于 自然语言处理中文本的预测。

通过将词语 One-hot 独热编码，然后用Embedding 矩阵嵌入。其简化的本质就是 Word2Vec，就是对一个文本的数据，学习每一个低维单词的表征。

早期将MLP这一类的神经网络直接通过在文本数据集中训练，然后进行预测的，就是神经语言模型。类似于计算机视觉中的图像分类，输入一个图片输出其对应的类别。

但神经语言模型通常都只能在特定的任务中表现出色，一旦换到新的任务，整个模型和数据一般就需要重新训练，需要较大的计算开销。也就是神经语言模型的一个问题，那就是——**缺乏通用性**！在数据集内可以有较好的预测效果，但无法泛化到数据集之外的数据！

### 预训练语言模型 （PLM）

为了让语言模型具有通用性，能够执行多种多样的任务，从 BERT、GPT 开始开发了一系列预训练语言模型。通过在大量语料上无监督训练，然后在特定下游任务或特定领域中微调。

这非常类似于我们现在接受的教育过程。从小学到初中我们接受九年义务教育，语数英物化生等等我们都要学，本质就类似于“预训练”让我们对整体的知识有一个较广方面的了解。而到了高等教育，大家都开始分化出各自的专业，这就是在特定的领域里进一步学习，也就是所说的”微调“。

BERT、GPT、GPT-2都属于预训练的语言模型。但传统的语言模型多存在缺乏背景知识、在复杂任务推理上表现不佳的问题。渐渐地，大语言模型就开始到来

### 大语言模型（LLM）

大语言模型的主要特征有

- 规模大：参数规模达到数十个 B，也就是数百亿，甚至万亿级别参数的模型
- 海量语料训练

相比传统语言模型，这极大扩展了模型的参数规模，以及数据的规模。而这也需要更加复杂的训练过程，同时也要求了更大的计算开销！GPT-3 训练一次就需要140万美元，而到了更大的语言模型 GPT-4，那就需要甚至上千万美元级别的成本！ChatGPT的部署，甚至大约需要数十万张 A100 的GPU！

---

## 大模型技术基础

2017年，谷歌的一篇 《Attention is all you need》提出的Transformer，通过注意力机制，在长序列处理上表现了独到的能力，更是让之称为了如今众多大语言模型的主流架构！

大语言模型，通常指的是参数量规模非常大的语言模型。现在主要是 Transformer 的 Decoder-only 架构。架构整体上分为：[1](@ref)：

1. Encoder-only：谷歌的 BERT、Meta AI 的 ALBERT
2. Decoder-only：这个是最主流的架构。Meta AI 的 Llama 系列、OpenAI 的 GPT 系列，以及谷歌的 Palm 等都属于 Decoder-only 架构。
3. Encoder-Decoder Hybird：就是编码器和解码器都有。Meta AI 的 BART、谷歌的 T5。

这三者本质都属于 Seq2Seq，也就是序列映射到序列。那么，构建一个大语言模型需要经历哪些技术过程呢？

### 构建大语言模型概览

实际上，整个一个大语言模型的过程，其实就很好比我们从小到大受教育的过程。大语言模型训练的大体过程，都是经过 Pre-training 预训练和 Post-training 后训练，紧接着在下游的任务上进行部署推理。

#### 预训练

预训练：Pre-training。在这一阶段，模型通过接触海量的文本数据，一般是通过 Next Token Prediction 这样 Autoregressive 的自回归方式进行的。这一过程主要是 Build Foundation，建立模型的基础能力。这一过程类似我们小学到高中接受义务教育———语数英物化生等各科知识都接触，对整个世间万物起码形成一个最基本的认知。

预训练阶段所消耗的数据一般多为数万亿个 Tokens（词元），需要耗费数千张甚至万张大规模的集群进行训练，而这一过程通常也需要持续数月才能完成。其目的是建立模型的基础能力。

预训练一般使用的数据包含网页、访谈内容、书本新闻、代码、科学数据等....但都与下游任务无关。这好比我们不管未来从事什么职业，都要通过九年义务教育学习语数外物化生等科目。

#### 后训练

后训练：Post-training。这一个阶段，则是为了赋予模型在执行特定任务的能力。这一过程通常包含

- SFT：监督微调。一般为在指定的下游任务数据集上进行微调。通过输入问题---理想输出的格式，对模型微调，从而提升模型对问答任务求解的能力。这类似我们在进入大学之后开始专业分化，会在一个特定的领域进行更深入的研究。
- RL：强化学习。 一般为 RLHF （基于人类反馈的强化学习），通过强化学习(PPO、DPO、GRPO)对齐人类偏好，使大模型的输出更加符合人类的期望、价值观等。这一过程一般需要训练奖励模型，通过人对模型的一系列输入---输出的行为进行打分，然后利用打的分数训练一个奖励模型，最后用强化学习的算法进一步微调。这更类似于我们走上社会，通过实习/社会实践，让我们更能够和社会时代接轨。

后训练相比之下就仅需要数百万个 Tokens 数据，相比预训练的规模少了很多数量级。所消耗的算力大约是几十张到几百张卡之间。

也就是说，现在我们提倡的“一专多能”，那么预训练就是通过大规模的数据集训练，赋予大模型“多能”的能力；而后训练则为了让大模型在多能的基础上训练出一个“专”。而对于对齐技术，则更相当于让我们和这个社会时代“接轨对齐”，而不是与时代脱轨脱节。

### 大模型规模扩展

我们都知道大语言模型有很大的参数量，也在很大批的数据集上经过了训练，那这个参数量不断扩展累加，会有什么样的效果呢？同样的，扩展我们训练数据集的规模，会怎么样？

OpenAI 的 GPT 系列模型 从最初代的 GPT-1 (117M), GPT-2 (1.5B) 到 GPT-3 (175B)，乃至 GPT-4 (1.8T)，模型的规模也是不断在扩大！模型参数量规模的扩大，我们也发现模型的性能不断提升，也更多展现出前驱模型可能没有的能力。这一规律实际上在 2020 年就早有所发现，下面我们来看一看：

#### Scaling Law

研究人员大语言模型的第一个发现就是扩展定律（Scaling Law），一般指的是通过扩展**模型规模、数据规模、计算算力**，可以让语言模型的能力显著提升。

假设你是一家大模型公司的研发人员，你遇到了如下问题：

- 我计划要训练一个模型，参数量大约 xx B，至少要多少数据可以满足
- 现在我们收集的数据集达到了 xx 亿 Tokens，能撑起多大的模型
- 有 xx 台 A100 / H100 的训练 GPU 算力资源，需要多大的数据、参数量？

也就是说，从算力 $C$、数据 $D$、模型大小（参数量） $N$ 这三个维度。这些问题的回答都需要通过大语言模型的 Scaling Law 得到指导。

OpenAI 团队通过实验，得到了损失函数值与参数规模 $N$、数据规模 $D$、以及计算算力 $C$ 大致呈现幂律关系（Power law）。也就是 $L(N)= K_1 N^{-\alpha}, L(D) = K_2 D^{-\beta}, L(C) = K_3 C^{-\gamma}$。也称为 KM Scaling Law。这里 $K_1, K_2, K_3$ 均为假设常数， $\alpha, \beta, \gamma$ 都大于0，通过这一关系不难看出，大语言模型的损失函数值，随着 $N, D, C$ 的不断扩大，呈现出指数性的递减，这就是幂律。损失函数值的降低，也对应着模型性能的提升。

当然，指的是 Decoder-Only 架构一类的模型。而 OpenAI 在 Scaling Law 中的实验还额外发现，对于模型本身，影响loss 结果最大的是参数量，反倒是在固定参数量的情况下，模型的具体结构却基本无关！（比如，可能会有较少的layer 但每一个内部 Embedding 的维度大；或者layer层数多深，但每个Embedding的维度小）。而计算量、模型参数量、数据大小这些，当固定另外两个变量时，与每个因素都呈现 Power law 幂律关系。

实际上，Scaling Law 中损失函数的核心公式在于分为可约与不可约两项之和----也就是

$L(x) = L_\infty + \left(\dfrac{x_0}{x}\right)^\alpha$。其中这里面 $L_\infty$ 是真实数据本身的熵 Entropy。这一项一般无法通过把模型参数大小 Scale up 而降低，因为数据本身可能存在一定噪音，而模型都是从数据中学习而更新参数；第二项 $(x_0/x)^\alpha$ 则是可以通过 Scale up 模型参数规模而降低的损失，这本质上是一个 KL 散度，也就是 $\text{KL}(\text{True}||\text{Model})$，表示真实分布和模型分布之间的 difference。

后来，Google Deepmind 为指导大语言模型充分利用算力进行优化训练，得出的 Scaling Law 表达形式为

$L(N, D)=E + AN^{-\alpha}+BD^{-\beta}$，这里 $\alpha, \beta >0$。

当然，当前可以使用的数据量也是有限。同时一旦模型扩展得太多，到后期的时候增益就基本上变得很小，临界饱和。

#### Emergent Ability

另外，大语言模型的另一个发现是 Emergent Ability 涌现能力。Scaling Law 指出，随着模型的规模增大，性能会逐步提升，是一个从有到优的过程。而另一个涌现能力，则是 从无到有的过程！也就是模型参数小时，在该任务性能不高，而当模型参数进一步扩大之后，在这一特定任务性能突然提升非常快！

那么，一般什么样的任务会表现出涌现能力？多数为多个步骤迭代的复杂任务：比如———一步步求解题目、语言指令跟随等。这里比较有代表性的能力包括

- 指令跟随：能够理解用户语言指令的意图，从而按照自然语言的指令执行任务。
- 上下文学习：给予语言模型一些任务示例，让语言模型“举一反三”，而不需要微调更新模型本身内部参数。这就类似我们“学习能力” 的提升，具有“快速学习新领域，现学现卖”的能力。
- 链式思维，逐步推理：也就是 CoT。像求解数学题等需要逐步推理的任务，引入中间的步骤，使模型能够一步一步更好得到最终答案。

总结而言，Scaling Law 和 Emerging Ability 是两种描述大模型累加规模带来效应的度量方法。前者偏于多个任务的平均损失，而后者偏于特定一个任务的性能提升。

如果 Scaling Law 好比我们成长过程，能够有更强的能力；那 Emergent Ability 就类似到一定阶段，会“突飞猛进”。就像我们的认知之中，进步总是“台阶”式的，每穿越一个瓶颈期，就等于 Emergent Ability 突然爆发了出来一次。或者说 Scaling law 是“量变”，而 “Emergent Ability” 则是质变。有时候，一个能力的突飞猛进顺便带动其他多个能力的提升效益。

---

## GPT 和 DeepSeek

从2022年 ChatGPT 的出世，再到如今 DeepSeek 的火爆，这些无一不是大语言模型发展的重要节点。我们来回顾一下 GPT 系列模型的 “前世今生”：

GPT，全称为 Generative-Pretrained Transformer。早在 2018 年，OpenAI 介绍了“生成式预训练”的模型，以 Transformer 为基础架构，通过在大规模的语料上进行训练，学习自然语言的模式和规律。

自从谷歌在 《Attention is all you need》中提出了 Transformer 架构后，就被从早期的 GPT-1 系列模型开始起采用了。在Transformer 系列模型之前，对语言这种有序列性的数据建模的网络以 RNN 为主，这中包含 GRU、LSTM 等变体。但 RNN 一类的模型有一个问题，那就是一旦数据序列变长，由于反复的传播会带来较为严重的梯度消失/爆炸问题。相比之下，Transformer 具有高效并行性，以及捕捉长序列依赖的能力，一度成为了从 GPT 开始乃至当今主流的语言模型架构。

### GPT系列模型

**GPT-1**

GPT-1 早在2018年就被OpenAI 团队提出，属于 Decoder-only 架构（Multi-head Attention 中有 Causal mask，更适合生成性的任务），参数量为 117M，但这个规模远远达不到大模型参数量的规模。在 GPT-1 中作者提出了生成式预训练的过程，其实是分为两个阶段的。

GPT-1 模型中含有多个堆叠的 Transformer block，其中每个block 包含 masked-multi-head attention 和 feed-forward 网络。通过微调GPT模型，可以针对特定的任务进行优化，例如文本生成、机器翻译和对话系统等。GPT-1 的“预训练”过程，其实是经历了预训练+微调的两阶段——

- 预训练：通过大量的无标注文本数据集，包含 Wikipedia 等各类网页文本，训练模型参数。
- 微调：在文本分类、文本相似度判别、选择等特定的下游任务中进行微调。

所以，GPT 的 P 虽然是 Pretrained，但其实并不是只有大规模预训练，其实内部也包含了微调的过程！

**GPT-2: "模型本身就是无监督的"**

GPT-2主要解决的问题，就是如何利用大规模未标注的自然语言文本，预训练一个通用的语言模型。

我们都知道有监督学习中的数据都配对一个标记label，但是大规模的语言文本要想标记起来，那将是极其困难耗时的过程！为了解决这个问题，让模型学习过程变为无监督，相比GPT-1 分类出多类任务，GPT-2 将这些多类的任务底层统一在一起，那就是——**预测下一个word**，且保证了预训练与下游任务一致，而不是再像 GPT-1 一样对于下游任务也分了很多类。

除了将模型无监督化，仅通过Prompt进行提示之外，GPT-2 相比 GPT-1 的规模也进一步得到了扩展——从原来的 117M 扩展到 1.5B。GPT-2除了在问答、文本分类的任务表现出色外，在生成文本的任务之中也非常出色！GPT-2 同时还初步展现出了 Few-shot 学习的能力，能够较容易地泛化执行新的任务。

**GPT-3：更大更强**

GPT-2 初步展现出了 Zero-shot 迁移到新任务的学习能力，而 GPT-3 基于此，主要解决的问题就是如何使一个预训练的语言模型，能够快速将知识迁移。GPT-3 仍然采用了 GPT-2 的模型架构，但是在规模上扩展了100余倍。GPT-3 的参数规模达到了 175 B，从 GPT-3 起算是正式迈入了大规模语言模型的行列。

GPT-3 还展现出了 Zero-shot 学习的能力，在零自然语言 Prompt 的提示情况下也具有一定的准确性性能，并且在给定 One-shot Prompt之后准确性性能飞速提升！GPT-3 展现出了 in-context learning 的能力！

为了让 GPT-3 模型更好服务于更加特定的下游任务，OpenAI 还基于 GPT-3 通过大量代码数据微调，得到 CodeX 模型(12B)；WebGPT 则专门用于与搜索引擎交互，通过多轮搜索和交互，从网页中提取信息并生成答案。

**从InstructGPT到ChatGPT**

InstructGPT 是 GPT-3 的基础上再通过提出 RLHF，利用基于人反馈的强化学习，将语言模型与人类的价值观进行对齐，从而提升模型的生成能力。其通过人标注的指令对 GPT-3 进行有监督微调（SFT），随后基于人类对模型输出的偏好排序，训练一个奖励模型，用于后续利用 PPO 算法对模型再一步强化学习微调。

RLHF 技术显著提升了模型在真实性、安全性和有用性方面的表现，即使参数规模更小，InstructGPT参数量为 1.3 B，也能比原来不使用人类偏好对齐手段的 GPT-3 (175 B) 在人类评估的性能更高！RLHF 技术的提出也为后续我们常用的 ChatGPT 的研发提供了基础！

GPT-3.5 在 GPT-3 的基础上，通过参数优化、性能提升和技术改进，提供了更高效、更精准的语言处理能力。而我们最初使用的 ChatGPT 的最初版，就是基于 GPT-3.5 通过进一步 SFT、RLHF 的微调，并进一步开发得到。ChatGPT 面向模型的对话进行优化，通过提升语言模型的多轮对话、与用户交互、错误纠正、拒绝不正当请求等能力，进行了进一步的改进，从而形成了我们如今使用的 ChatGPT。

**GPT-4到GPT-4o**

进一步，2023年初，GPT-4 的出现在继 ChatGPT 开放 GPT-3.5 之后也成为了热点！当聊天的任务变得复杂时，GPT-4 展现出了比 GPT-3.5 更强的可靠性和准确性，并且对任务指令的处理也进一步更加细化。GPT-4 参数量达到了1.8T，相比 GPT-3 也进一步大了10余倍，训练的数据语料规模更是达到了 13T tokens！

更有趣的是，GPT-4 还支持图像等多模态的输入。在有GPT-4之后，我们发现模型可以上传图片，而模型本身对图片也展现出了很强的理解能力。

GPT-4o 将参数量优化到 200 B，虽然相比 GPT-4 有所减少，但是过算法优化和高效设计，其性能在特定任务中表现更加出色。

**GPT o系列**

2024年9月，GPT 推出o1 系列！相较于前面的几代 GPT，GPT o1起模型开始拥有了 Chain of Thought 链式思维的能力。模型可以将复杂的任务问题一步一步拆解细化，通过不断地加深思考，在长序列任务的推理能力上，性能有了极其大幅的提升！

像人类一样，在遇到问题时，都会慢慢深入思考，一步一步将问题细化拆解，逐个击破... GPT o1 在一些长任务的完成上，就像做数学题一步一步推理这种任务，极大超越了以往的 4o 模型！

o1 的出现，代表着大模型领域从单纯的生成，不断走向深度推理；从简单的生成式任务，到复杂的推理任务的不断推广深化。

### DeepSeek系列模型

今年的春节，DeepSeek 系列模型的崛起，剑指o1的水平，突然火爆了全世界！

OpenAI 从 GPT-1 到如今的 o1 模型虽然不断发展，但是始终没有将模型开源，还曾经封了很多国家地区的API！"OpenAI 不Open"！而 DeepSeek 系列的模型，特别是如今 R1 一经推出，瞬间引起了世界的轰动！

DeepSeek 背后是幻方量化公司！同样的，DeepSeek 有什么技术的发展演变？又是怎么成长起来的？

- HAI-LLM：DeepSeek 的发展始于幻方量化研发的HAI—-LLM，这是一个于2023年6月发布的大规模深度学习的主力框架。DeepSeek 系列的模型一直以 HAI-LLM 为训练框架。
- DeepSeek-LLM (2024.1)：这其实就是 DeepSeek V1 发布的第一版本的语言大模型，其中探索了 Scaling law 的分析，对数据语料进行了进一步的优化，预估了超参数的性能。其性能超越了GPT-3.5
- DeepSeek Coder 加入大量代码数据，DeepSeek Math 收集了大量的数学数据，Deepseek VL 加入了一定图文数据。
- 其中，Math 的报告中提出了强化学习的 GRPO 算法，其在 PPO 中将价值函数去掉，并且通过将策略网络的输出分为多个组，得到多个组的数据。
- DeepSeek V2 (2024.5)：V2 版本采用了 MoE（混合专家）架构，一定程度上优化了参数效率，并对应提出了相应的稳定性策略；同时V2 版本提出了 MLA，将注意力的计算压缩到 latent 隐空间中进行，降低了计算开销，这一技术还被加速为 FlashMLA 开源出。对应的 V2 版本也出了一个coder 代码助手，性能达到约 GPT-4 Turbo。

而到了如今的 DeepSeek V3 和 R1，也就是今年春节登上全世界热搜的模型！与传统的效率优先不同，Deepseek 系列更看重“成本”与“效率“的平衡，相比于 OpenAI 等靠堆叠卡和模型的方式，DeepSeek 在本质上**开拓了一条新的道路——优化模型架构、训练方法**----能够需要更少的计算资源而训练出同样优越性能的大模型！

DeepSeek V3 (2024.12)：V3 同样基于了 V2 的 MoE 架构，延续 MLA 高效的注意力机制。V3的高效技术在于：

- 模型架构上，仍然应用 MoE 的架构，并且提出了 MTP (Multi-Token Prediction)，一次预测多个token，增强并行性，减少迭代次数，并降低累计误差。
- 并行策略：大量使用 EP 并行策略代替 TP
- 内存优化：使用 FP 8 低精度训练，并且在内部模型量化上也采用了分块、高精度累加等量化策略。

而 DeepSeek-R1 以 V3 为基础模型，其性能超越了 OpenAI 的 o1 水平！DeepSeek R1 的出现，也让大语言模型更一步进入了深度思考的时代！DeepSeek R1 这篇工作本身也包含了如下几个过程：

- R1-Zero：以DeepSeek V3 为 Base，跳过 SFT，直接进入强化学习阶段微调，使用 GRPO 的 RL 算法，表现出自我进化、反思的更长更复杂的链式思维能力，但是存在RL初期不稳定的问题。
- R1：DeepSeek V3 + Cold Start SFT + 初期 RL + 能力迁移 SFT + 全场景 RL 对齐。先通过冷启动 SFT 确保后面的 RL 稳定，然后通过初期的 RL 激发模型自主推理的能力；紧接着将 RL 涌现出的推理能力，再通过 SFT，迁移到更多任务方面类型（如写作、扮演角色等），最后再多个场景下RL对齐，解决模型的“偏科”问题。
- R1 蒸馏给小模型：R1 的数据 在学生模型上 SFT，用教师模型R1的数据去SFT小模型，使小模型能力得到提升，且优于小模型+RL本身。

想必很多人也都听说了 Aha Moment，而我也是随着 DeepSeek 的火爆第一次了解。这是模型自身学习的过程中，不断复盘评估，能够自我反思并进行纠错。这就好像我们上课老师指点之后，突然一瞬间明白了的那种恍然大悟的感觉！Aha Moment也就是模型在“推理时思考”的表现，也会表现出模型不断检查是否有误、会自身验证纠错等能力。类似于我们做题或者做任务当中——
- “等等，我好像知道可能是哪里的问题！我回去排查一下.....就是这里的问题！”
- “啊怪不得！我人傻了，把这个加号当减号了，我说怎么卡在这块一直出错！”

R1-Zero 证明了通过 RL 可以让模型释出这种能力，就像我们前面比喻 RL 微调犹如社会实践/实习一样，通过实战亲临能够有更深刻的体会与学习！当然，R1-Zero 存在的问题：
- 其一：​语言混合，例如推理过程中常出现中英文混杂的问题。
- 其二：格式不稳定，部分输出仍存在标签缺失或错位。

R1 通过加入少量高质量数据作为冷启动，其收集少量的 CoT 数据微调模型作为RL的起点。其目的旨在通过引入少量高质量数据引导模型建立基础推理框架，解决R1-Zero 中存在的直接 RL 导致初期的不稳定性问题。核心目标也即

- 让模型规范输出格式，例如：让推理过程放在 `<think>` 中，而答案放在 `<answer>` 中
- 建立模型的基本推理逻辑，比如要让模型解答数学题，那就要引导模型一步一步将问题细化拆分，模仿 CoT 一步步推理得到答案
- 降低模型初期随机试错。

假设你要去一个地方社会实践/实习，那社会实践前的动员大会，初期召集开会的作用，其实就都像这个冷启动，是为了后面的强化学习更好进行做一定的过渡铺垫。这其实就类似 R1 的冷启动 SFT，然后开始初期 RL。初始强化学习，也类似于一般是第一次或者头几次实习，会让你有一个从0到1对走上现实社会中，应该怎么发现问题，怎么解决问题的思路、思考过程的了解。

当然，这只是一个初期，初步的实习可以让你初步接触一些专业技能与思考方向；但社会经验等一些深远复杂的，还需要更加综合多元的能力参与进来———人毕竟是一个多面体。我们初步涌现出解决问题的能力之后，还要将这一种能力迁移到生活中的方方面面（如表达、组织、统筹、文笔等），这就是需要能力迁移 SFT，通过初期 RL 获得的经验复用，进一步去 SFT，然后再在这些全方位进行全场 RL 进一步对齐，提升综合能力。

最后，DeepSeek R1 还蒸馏给小模型如 Llama、Qwen等等。在蒸馏过程仅使用 SFT，而没进行 RL。这也展现出蒸馏技术的有效性！（虽然使用RL微调可以增强模型之性能）

---

## References

> [1](@ref): [大语言模型的三种架构](https://zhuanlan.zhihu.com/p/642923989)
> [2](@ref): [解析大模型中的Scaling Law](https://zhuanlan.zhihu.com/p/667489780)
> [3](@ref): [DeepSeek 详解](https://mp.weixin.qq.com/s?__biz=MzkzNjMzNTM4NQ==&mid=2247493139&idx=1&sn=add950df5399007e4e86bc8a9e1d115f&chksm=c3316d3dc40cd1e56b5e08e8f74cf9863889e1fe92789d576c60f578abeee46b1b51b65295cc#rd)
> [4](@ref): [一文搞懂DeepSeek-结合冷启动的强化学习](https://mp.weixin.qq.com/s?__biz=MzkzMTEzMzI5Ng==&mid=2247493580&idx=1&sn=115a39f16d72d7f7e1d71ed3ea28dc5e&chksm=c3466b7e0b30e30644ce6d1256f69efa2d82f4bd91c2c4dab3cafb46b8b7d9622d289e73ee97#rd)
