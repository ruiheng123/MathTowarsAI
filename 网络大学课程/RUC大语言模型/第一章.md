# 第一章 大语言模型初识

从2022年末ChatGPT的横空出世，到如今DeepSeek再度进入大众视野，大语言模型已经不断深入各个领域，走进千家万户。大语言模型的迭代可谓层出不穷，最初从 OpenAI 的GPT-2 GPT-3 、Google 的 T5、Meta AI 的 Llama；国内的 GLM、百川智能、零一万物、通义千问、文心一言、书生浦语、智谱清言......可以说，从2023年起，ChatGPT问世之后中国也爆发了“百模大战”。首先我们要了解，语言模型是如何发展到如今的？

## 语言模型的发展历程

在1990年代，那时神经网络 MLP 和 BP 反向传播算法的刚被提出，机器学习在那时仍然主要在 PCA、SVM 这些数学统计方法集中，还未有如今深度学习发展的技术。实际上在那个时代，传统统计机器学习方法就已经开始应用于对语言数据进行统计建模。

语言模型的目标建模本质上就是拟合一个概率分布，假设有一个词汇表 $V$ 。对于一句含有 $m$ 个词语的话，我们可以表示为 $w_1 w_2 \cdots w_m$，而语言模型本质上就是要建模拟合概率分布。

然而，由于现实中语言一句话长度往往很长，对于汉字更是数以万计，如果要想直接通过 对 $w_1w_2\cdots w_m$ 建模，此时语言模型的存储量和参数量就会极其大！

（例如：可以想象生成图片，对于 28 * 28 = 784 的 MNIST 图像大小，图像的值是 Binary 的二进值。要想把每个情况都考虑，那基本上要达到 $2^{784}$ 的规模，这个规模的极其大的！！而语言的长度、汉字词语的组合更是极其复杂！）

### 统计语言模型 （SLM）

我们都知道现在的大语言模型都是使用了自回归方式预测，也就是常见的“Next Token Prediction”，说白了点，就是“**前面所有的，预测下一步**”。本质上也就是

$P(w_1 w_2 \cdots w_m) = \prod_{i=1}^m P(w_i|w_{<i})$

这就是我们通常说的链式法则，但如果基于过去的单词个数较多就会增加计算开销。于是，n-gram 模型基于 Markov 假设。也就是说这里面

$P(w_i | w_{<i}) = P(w_i|w_{i-1-n : i})$

当 $n$ 取1时，就是一个 Markov 过程。

这本质上属于统计机器学习模型，其具备一定的生成能力。那么这个模型是如何去估计的？

- 基于频率的估计：假设有一个词典，里面记录了很多词语以及句子。如果 “我 非常 喜欢” 这段出现了 100 次，下一句 ” 我 非常 喜欢 你“ 出现了 10 次，”我 非常 喜欢 学习“ 出现了20 次，则根据条件概率公式我们有: $P(你 | 我 非常 喜欢) =  P( 我 非常 喜欢 你)  /  P(我 非常 喜欢)=0.1$
- 上面的本质就是统计语言模型的作用，其基于频率去估计条件概率。那在下一次大致就可以达到 如果说 ”我非常喜欢“，则 10% 的概率会说”我非常喜欢你“.....

但这一方法有什么问题呢？

1. 需要有一个词典的要求！如果这个词典太大了，那存储开销和计算、搜索查询过程的开销会变得非常大。
2. 如果出现了一段话不在词典里，例如“我非常喜欢数学“ 这句话，没有出现在这个词典里，那这个概率就是0！而语言组合千变万化很容易出现这个情况！
3. 早期解决的方法之中，一种是增加一个平滑，就是让未出现的一段词语分配一个较小的权重，使得其概率权值不是0。也就是 Laplace Smooth。另一种是回退，本质上就是把单词缩小一点，扩大一些范围。比如“我 非常 喜欢” 假设出现很少，那可以把这个条件改为 ”非常 喜欢“，去掉 "我"，这样，"非常 喜欢 老师讲的...." 就可能出现。通过回退一步，略扩大一些范围。

我们发现，早期的统计语言模型，在没有深度网络时，都需要靠收集统计大量的语言数据。而我们能够收集到的语言数据更是有限，这在浩瀚如海的文字中甚至只不过是 “大海捞针” 的程度。**本质上仍然是不能解决数据稀缺的问题。**

### 神经语言模型 （NLM）

深度神经网络开始引入之后，AI 进入了深度学习时代，神经网络在计算机视觉（CV）领域中，图像分类、目标检测中得到了广泛应用。而在早期自然语言处理（NLP）也发挥了一定作用。

神经网络说到底本质更像一个“拟合函数”。早期利用多层感知机（MLP），也就是把自然语言的单词，映射到一个向量。例如

- 我 --- [0.1, 0.8, 0.9, 0.4, ..]  喜欢 --- [ 0.4, 0.9, 0.5, ....]。
- 通过这种映射方式，把语言从自然语言空间，转换到数字空间，也就是能够被计算机计算，从而达到一个预测的目的

熟悉深度学习的大家都知道 RNN 循环神经网络在早期对带有序列性的数据处理具有一定的优势，其可以通过前序的数据的隐空间特征，结合当前数据的输出预测下一个时间步长的数据。RNN 早起就被应用于 自然语言处理中文本的预测。

通过将词语 One-hot 独热编码，然后用Embedding 矩阵嵌入。其简化的本质就是 Word2Vec，就是对一个文本的数据，学习每一个低维单词的表征。

早期将MLP这一类的神经网络直接通过在文本数据集中训练，然后进行预测的，就是神经语言模型。类似于计算机视觉中的图像分类，输入一个图片输出其对应的类别。

但神经语言模型通常都只能在特定的任务中表现出色，一旦换到新的任务，整个模型和数据一般就需要重新训练，需要较大的计算开销。也就是神经语言模型的一个问题，那就是——**缺乏通用性**！在数据集内可以有较好的预测效果，但无法泛化到数据集之外的数据！

### 预训练语言模型 （PLM）

为了让语言模型具有通用性，能够执行多种多样的任务，从 BERT、GPT 开始开发了一系列预训练语言模型。通过在大量语料上无监督训练，然后在特定下游任务或特定领域中微调。

这非常类似于我们现在接受的教育过程。从小学到初中我们接受九年义务教育，语数英物化生等等我们都要学，本质就类似于“预训练”让我们对整体的知识有一个较广方面的了解。而到了高等教育，大家都开始分化出各自的专业，这就是在特定的领域里进一步学习，也就是所说的”微调“。

BERT、GPT、GPT-2都属于预训练的语言模型。但传统的语言模型多存在缺乏背景知识、在复杂任务推理上表现不佳的问题。渐渐地，大语言模型就开始到来

### 大语言模型（LLM）

大语言模型的主要特征有

- 规模大：参数规模达到数十个 B，也就是数百亿，甚至万亿级别参数的模型
- 海量语料训练

相比传统语言模型，这极大扩展了模型的参数规模，以及数据的规模。而这也需要更加复杂的训练过程，同时也要求了更大的计算开销！GPT-3 训练一次就需要140万美元，而到了更大的语言模型 GPT-4，那就需要甚至上千万美元级别的成本！ChatGPT的部署，甚至大约需要数十万张 A100 的GPU！

## 大模型技术基础

2017年，谷歌的一篇 《Attention is all you need》提出的Transformer，通过注意力机制，在长序列处理上表现了独到的能力，更是让之称为了如今众多大语言模型的主流架构！

大语言模型，通常指的是参数量规模非常大的语言模型。现在主要是 Transformer 的 Decoder-only 架构。架构整体上分为：[1](@ref)：

1. Encoder-only：谷歌的 BERT、Meta AI 的 ALBERT
2. Decoder-only：这个是最主流的架构。Meta AI 的 Llama 系列、OpenAI 的 GPT 系列，以及谷歌的 Palm 等都属于 Decoder-only 架构。
3. Encoder-Decoder Hybird：就是编码器和解码器都有。Meta AI 的 BART、谷歌的 T5。

这三者本质都属于 Seq2Seq，也就是序列映射到序列。那么，构建一个大语言模型需要经历哪些技术过程呢？

### 构建大语言模型概览

实际上，整个一个大语言模型的过程，其实就很好比我们从小到大受教育的过程。大语言模型训练的大体过程，都是经过 Pre-training 预训练和 Post-training 后训练，紧接着在下游的任务上进行部署推理。

#### 预训练

预训练：Pre-training。在这一阶段，模型通过接触海量的文本数据，一般是通过 Next Token Prediction 这样 Autoregressive 的自回归方式进行的。这一过程主要是 Build Foundation，建立模型的基础能力。这一过程类似我们小学到高中接受义务教育———语数英物化生等各科知识都接触，对整个世间万物起码形成一个最基本的认知。

预训练阶段所消耗的数据一般多为数万亿个 Tokens（词元），需要耗费数千张甚至万张大规模的集群进行训练，而这一过程通常也需要持续数月才能完成。其目的是建立模型的基础能力。

预训练一般使用的数据包含网页、访谈内容、书本新闻、代码、科学数据等....但都与下游任务无关。这好比我们不管未来从事什么职业，都要通过九年义务教育学习语数外物化生等科目。

#### 后训练

后训练：Post-training。这一个阶段，则是为了赋予模型在执行特定任务的能力。这一过程通常包含
 
- SFT：监督微调。一般为在指定的下游任务数据集上进行微调。通过输入问题---理想输出的格式，对模型微调，从而提升模型对问答任务求解的能力。这类似我们在进入大学之后开始专业分化，会在一个特定的领域进行更深入的研究。

- RL：强化学习。 一般为 RLHF （基于人类反馈的强化学习），通过强化学习(PPO、DPO、GRPO)对齐人类偏好，使大模型的输出更加符合人类的期望、价值观等。这一过程一般需要训练奖励模型，通过人对模型的一系列输入---输出的行为进行打分，然后利用打的分数训练一个奖励模型，最后用强化学习的算法进一步微调。这更类似于我们走上社会，通过实习/社会实践，让我们更能够和社会时代接轨。

后训练相比之下就仅需要数百万个 Tokens 数据，相比预训练的规模少了很多数量级。所消耗的算力大约是几十张到几百张卡之间。

也就是说，现在我们提倡的“一专多能”，那么预训练就是通过大规模的数据集训练，赋予大模型“多能”的能力；而后训练则为了让大模型在多能的基础上训练出一个“专”。

### 大模型规模扩展

我们都知道大语言模型有很大的参数量，也在很大批的数据集上经过了训练，那这个参数量不断扩展累加，会有什么样的效果呢？同样的，扩展我们训练数据集的规模，会怎么样？

#### Scaling Law

研究人员大语言模型的第一个发现就是扩展定律（Scaling Law），一般指的是通过扩展**模型规模、数据规模、计算算力**，可以让语言模型的能力显著提升。OpenAI 通过实验，得到了损失函数值与参数规模 $N$、数据规模 $D$、以及计算算力 $C$ 大致呈现幂律关系（Power law）。也就是 $L(N)= K_1 N^{-\alpha}, L(D) = K_2 D^{-\beta}, L(C) = K_3 C^{-\gamma}$

这里 $K_1, K_2, K_3$ 均为假设常数， $\alpha, \beta, \gamma$ 都大于0，通过这一关系不难看出，大语言模型的损失函数值，随着 $N, D, C$ 的不断扩大，呈现出指数性的递减，这就是幂律。损失函数值的降低，也对应着模型性能的提升。




## References

> [1](@ref): [大语言模型的三种架构](https://zhuanlan.zhihu.com/p/642923989)
