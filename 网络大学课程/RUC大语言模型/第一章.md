# 第一章 大语言模型初识

从2022年末ChatGPT的横空出世，到如今DeepSeek再度进入大众视野，大语言模型已经不断深入各个领域，走进千家万户。大语言模型的迭代可谓层出不穷，最初从 OpenAI 的GPT-2 GPT-3 、Google 的 T5、Meta AI 的 Llama；国内的 GLM、百川智能、零一万物、通义千问、文心一言、书生浦语、智谱清言......可以说，从2023年起，ChatGPT问世之后中国也爆发了“百模大战”。首先我们要了解，语言模型是如何发展到如今的？

## 语言模型的发展历程

在1990年代，那时神经网络 MLP 和 BP 反向传播算法的刚被提出，机器学习在那时仍然主要在 PCA、SVM 这些数学统计方法集中，还未有如今深度学习发展的技术。实际上在那个时代，传统统计机器学习方法就已经开始应用于对语言数据进行统计建模。

语言模型的目标建模本质上就是拟合一个概率分布，假设有一个词汇表$V$。对于一句含有$m$个词语的话，我们可以表示为 $w_1 w_2 \cdots w_m$，而语言模型本质上就是要建模拟合概率分布。

然而，由于现实中语言一句话长度往往很长，对于汉字更是数以万计，如果要想直接通过 对 $w_1w_2\cdots w_m$建模，此时语言模型的存储量和参数量就会极其大！

（例如：可以想象生成图片，对于 28 * 28 = 784 的 MNIST 图像大小，图像的值是 Binary 的二进值。要想把每个情况都考虑，那基本上要达到 $2^{784}$的规模，这个规模的极其大的！！而语言的长度、汉字词语的组合更是极其复杂！）

### 统计语言模型 （SLM）

我们都知道现在的大语言模型都是使用了自回归方式预测，也就是常见的“Next Token Prediction”，说白了点，就是“**前面所有的，预测下一步**”。本质上也就是

$P(w_1 w_2 \cdots w_m) = \prod_{i=1}^m P(w_i|w_{<i})$

这就是我们通常说的链式法则，但如果基于过去的单词个数较多就会增加计算开销。于是，n-gram 模型基于 Markov 假设。也就是说这里面

$ P(w_i | w_{<i}) = P(w_i|w_{i-1-n : i} ) $

当 $n$ 取1时，就是一个 Markov 过程。

这本质上属于统计机器学习模型，其具备一定的生成能力。那么这个模型是如何去估计的？

- 基于频率的估计：假设有一个词典，里面记录了很多词语以及句子。如果 “我 非常 喜欢” 这段出现了 100 次，下一句 ” 我 非常 喜欢 你“ 出现了 10 次，”我 非常 喜欢 学习“ 出现了20 次，则根据条件概率公式我们有
  $P(你 | 我 非常 喜欢) =  P( 我 非常 喜欢 你)  /  P(我 非常 喜欢)=0.1$
- 上面的本质就是统计语言模型的作用，其基于频率去估计条件概率。那在下一次大致就可以达到 如果说 ”我非常喜欢“，则 10% 的概率会说”我非常喜欢你“.....

但这一方法有什么问题呢？

1. 需要有一个词典的要求！如果这个词典太大了，那存储开销和计算、搜索查询过程的开销会变得非常大。
2. 如果出现了一段话不在词典里，例如“我非常喜欢数学“ 这句话，没有出现在这个词典里，那这个概率就是0！而语言组合千变万化很容易出现这个情况！
3. 早期解决的方法之中，一种是增加一个平滑，就是让未出现的一段词语分配一个较小的权重，使得其概率权值不是0。也就是 Laplace Smooth。另一种是回退，本质上就是把单词缩小一点，扩大一些范围。比如“我 非常 喜欢” 假设出现很少，那可以把这个条件改为 ”非常 喜欢“，去掉 "我"，这样，"非常 喜欢 老师讲的...." 就可能出现。通过回退一步，略扩大一些范围。

我们发现，早期的统计语言模型，在没有深度网络时，都需要靠收集统计大量的语言数据。而我们能够收集到的语言数据更是有限，这在浩瀚如海的文字中甚至只不过是 “大海捞针” 的程度。**本质上仍然是不能解决数据稀缺的问题。**

### 神经语言模型 （NLM）

深度神经网络开始引入之后，AI 进入了深度学习时代，神经网络在计算机视觉（CV）领域中，图像分类、目标检测中得到了广泛应用。而在早期自然语言处理（NLP）也发挥了一定作用。

神经网络说到底本质更像一个“拟合函数”。早期利用多层感知机（MLP），也就是把自然语言的单词，映射到一个向量。例如

- 我 --- [0.1, 0.8, 0.9, 0.4, ..]  喜欢 --- [ 0.4, 0.9, 0.5, ....]。
- 通过这种映射方式，把语言从自然语言空间，转换到数字空间，也就是能够被计算机计算，从而达到一个预测的目的

熟悉深度学习的大家都知道 RNN 循环神经网络在早期对带有序列性的数据处理具有一定的优势，其可以通过前序的数据的隐空间特征，结合当前数据的输出预测下一个时间步长的数据。RNN 早起就被应用于 自然语言处理中文本的预测。

通过将词语One-hot 独热编码，然后用Embedding 矩阵嵌入。其简化的本质就是 Word2Vec，就是对一个文本的数据，学习每一个低维单词的表征。

早期将MLP这一类的神经网络直接通过在文本数据集中训练，然后进行预测的，就是神经语言模型。类似于计算机视觉中的图像分类，输入一个图片输出其对应的类别。

但神经语言模型通常都只能在特定的任务中表现出色，一旦换到新的任务，整个模型和数据一般就需要重新训练，需要较大的计算开销。也就是神经语言模型的一个问题，那就是——**缺乏通用性**！在数据集内可以有较好的预测效果，但无法泛化到数据集之外的数据！

### 预训练语言模型 （PLM）

为了让语言模型具有通用性，能够执行多种多样的任务，从BERT、GPT开始开发了一系列预训练语言模型。通过在大量语料上无监督训练，然后在特定下游任务或特定领域中微调。

这非常类似于我们现在接受的教育过程。从小学到初中我们接受九年义务教育，语数英物化生等等我们都要学，本质就类似于“预训练”让我们对整体的知识有一个较广方面的了解。而到了高等教育，大家都开始分化出各自的专业，这就是在特定的领域里进一步学习，也就是所说的”微调“。

BERT、GPT、GPT-2都属于预训练的语言模型。但传统的语言模型多存在缺乏背景知识、在复杂任务推理上表现不佳的问题。渐渐地，大语言模型就开始到来

### 大语言模型（LLM）

大语言模型的主要特征有

- 规模大：参数规模达到数十个 B，也就是数百亿，甚至万亿级别参数的模型
- 海量语料训练

相比传统语言模型，这极大扩展了模型的参数规模，以及数据的规模。而这也需要更加复杂的训练过程，同时也要求了更大的计算开销！GPT-3 训练一次就需要140万美元，而到了更大的语言模型 GPT-4，那就需要甚至上千万美元级别的成本！ChatGPT的部署，甚至大约需要数十万张A100 的GPU！


## 大模型技术基础

2017年，谷歌的一篇 《Attention is all you need》提出的Transformer，通过注意力机制，在长序列处理上表现了独到的能力，更是让之称为了如今众多大语言模型的主流架构！

大语言模型，通常指的是参数量规模非常大的语言模型。现在主要是Transformer 的 Decoder-only架构。
