# 第二章 大语言模型架构

## Transformer 模型介绍

2017年，谷歌的一篇 《Attention is all you need》提出的Transformer ，以其在处理长序列信息的高性能，在语言模型领域颠覆了传统 RNN、LSTM 等序列模型。要说 Transformer 的很多细节，那可谓是各大大厂面试的 “重中之重”！所以我们来捋一捋，Transformer 你真的了解透了吗？

### 注意力机制

假设我们看一段较长的文字信息，比如：”虽然 xxx，xxx....xxx，.....，但是 yyy“，尽管前面“虽然”这块说了一大堆，但我们都知道这种转折性的文字，主要的信息在 “但是” 之后。因此我们会把我们看文字的注意力集中在后面 “但是” 这一块。

假设我们人眼识别一张图片中的物体，那么我们会把我们更多的注意力放在这个物体上，而相比之下物体所处的背景没有那么重要，也就自然不会调动更多的注意力去注意之。

Transformer 中的注意力模块，其设计的灵感也是来源于此！

想必看过深度学习的人在看Transformer 的时候，也都必然多少看过那么一两眼 QKV 这个东西！Query（查询）、Key（键）、Value（值）。更深一点的也都知道

$\text{Attn}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V$

我们逐个元素分析，看一看里面是什么含义？实际上，当我们看到 Key 和 Value 这二者会自然而然想到 Python 的字典数据类型，因为其里面就是 `{key: value}` 的配对pair格式。

注意力机制，其实本质上可以视为**通过相似度的查表**：现在你手上有一个 查询的 query 向量 $q$，累计假设 $N$ 个 Key-Value 配对，也就是 $(k_i, v_i)_{i=1}^N$。而这一“查表”的计算过程可以如下表示：
- 第一步是计算你要查询 $q$ 和每一个 $k_i$ 的相似度，也就是计算 $q$ 和 $k_i$ 的点积（余弦相似度）。
- 第二步是对这些相似度进行归一化，让其和为1，也就是 $\text{Softmax}$ 操作，就是 $w_i = \frac{\exp(q^T k_i)}{\sum_{i=1}^N \exp(q^Tk_i)}$。
- 第三步是将归一化的相似度得分 $(w_i)_{i=1}^n$作为权值与 $v_i$加权求和 $\sum_{i=1}^N w_i v_i$得到最终的输出。

如果我们实现 Transformer，特别是自注意力 Self Attention 或者多头自注意力 Self Multi-head Attention，一般都是确保 Q, K, V 会被 Embed 到 `[.., N, dim]` 的空间之内。但实际上：
- Query 的个数 `N_q` 可以不等于 Key 的个数 `N_k`，一般这种情况多出现于 Cross Attention；但是因为 Key 和 Value 配对，Key 和 Value 的个数必须相等。
- 为了保证 dot product 点积可以运算，Query、Key、Value 三者内部 Embedding 的维度 `dim` 必须相等。

实际上，我们可以用一个例子来比喻一下这个过程是什么样子，也会引用一些我们前面举得看文章的注意力。假设我们做一个英语阅读，有若干道细节理解题，我们要做的是找到文章中与问题问句相关的句子，也就是哪里讲了这个问题问的东西，然后即可找到答案返回。

- 题目的个数是 $N_q$ 也就是有 $N_q$个在文中查询的目标 Query，我们要带着问题去与原文匹配找到答案。假设文章有 $N_k$ 句，每一句其实都暗含对应一个问题，也都给出了这个答案。那么所讲问题其实就是 Key，而对应配对的答案是 Value。
- 比如：一个题目问题是："Bob 出生于哪一年"，这个题目问题就是 Query。文章中一句 "1998年 Bob 出生"，这句话隐含的问题是 "Bob 出生于哪一年？" 就是 Key；对应的答案 "1998年" 就是 Value。
- 我们要计算题目问题和文章中每一句所讲问题的相似度，也就是计算 Query 和 Key 的余弦相似度（点积）。之后，对这个问题我们的相似度得分进行归一化，也就是 Softmax 操作。相似度得分越大的分配的 Softmax 分数也越大。实际上，在 Softmax 出来一个概率分布的时刻，我们分配的权重就会反映出——集中在和题目问题最相关的句子上！
- 最后，我们将归一化的相似度得分 $(w_i)_{i=1}^{N_k}$ 作为权值与每个答案 $v_i$ 加权求和得到最终可能的答案。

所以说，实际上我们的 Attention 注意力机制，其实本质上可以作为一个通过相似度进行查询的过程。查询到相似度最高的地方，分配的注意力得分就越集中在那附近，也就类似我们会把注意力分配到最相关的位置！



### Transformer 架构

Transformer 的核心模块，就是注意力机制。相比于传统的 CNN 卷积操作，整个网络架构就是注意力机制。Transformer 包含一个 Encoder 编码器和 Decoder 解码器。
- Encoder 编码器将输入序列变为 隐空间 latent 特征
- Decoder 解码器将隐空间 latent 特征解码为输出序列

#### Encoder 
假设Encoder有 $N$ 个堆叠的块，每块有2个子模块：一个多头注意力(MHA)层，和一个前馈(FFN)层。在 MHA 和 FFN 后都有一个 Layer Normalization 和残差连接。这个属于 Post-Norm。假设第 $L$层输入的为 $x_{L-1}$，则 Encoder 内每个模块的传播过程是
1.  $x_{L-1}=\text{LN}(\text{MHA}(x_{L-1})+x_{L-1})$
2.  $x_L=\text{LN}(\text{FFN}(x_{L-1})+x_{L-1})$
这样反复经过 $N$ 次，也就是通过 $N$ 个这样的层，最后输出 $x_{\text{out}}$。在第一层前面对于最初的输入 $x$ 先通过词嵌入 Embedding，然后位置编码 PE。

#### Decoder

Encoder 里有 $N$个块，假设从第 $L$ 个块输出的结果为 $x_L$。

Decoder 同样也有 $N$ 个块，里面的其中一个 Attention 模块要接受来自 Encoder 对应那一块的输出。Decoder 里包含多头注意力。其中一般第一个多头注意力是自注意力，但含有 Causal Mask 因果掩码以防止模型预先知道未来信息；在第二个多头注意力模块是 Encoder-Decoder 的交叉注意力，也就是 Query 为 Decoder 中的输入，但是 Key-Value 接受的是来自 Encoder 对应那层的输出。

这样，假设 Encoder 第 $L$ 层输出的结果是 $x_L$，Decoder 输入到第 $L$ 层的输入为 $y_{L-1}$，则 Encoder-Decoder二者混合的时候模型传播的过程为：
1.  $y_{L-1} = \text{LN}(\text{MaskMHA}(y_{L-1})+y_{L-1})$
2.  $y_{L-1} = \text{LN}(\text{CrossMHA}(y_{L-1}, x_L)+y_{L-1})$
3.  $y_{L} = \text{LN}(\text{FFN}(y_{L-1})+y_{L-1})$
Decoder 仍然是 Post-Norm，也就是在执行完了注意力机制之后进行 LayerNorm 归一化，且也都有 Residual 残差连接。同样在第一层前面对于最初的输入 $y$ 先通过词嵌入 Embedding，然后位置编码 PE。

当然，需要注意的是，现在的很多大语言模型都是 Decoder-Only 架构，这时候 Encoder 部分是没有的，因此第二个式子就不是 和来自 Encoder 那边的输出做 Cross MHA，而是和第一个式子一样使用含有 Causal Mask 的 MaskedMHA。

现在的大语言模型主要都是 Decoder-Only 架构，主要其中一点就是 Decoder 部分的MHA有 Causal mask，防止模型预先知道未来的信息，是单向注意力；而 Encoder 部分的 MHA 没有 Causal mask，也就是注意力是双向的，比如 BERT，其注意力就是双向的。

Encoder only 的架构模型，就更适合做文本理解处理等任务，而不适合文本生成；Encoder-Decoder 都有的 Hybird 需要训练一个好的Encoder，这也大大增加了需要的计算开销。而Decoder-Only 在计算上有更高的效率和更快的推理速度。另外，Decoder-only 的架构模型在无需额外微调也就是 Zero-shot 下，能够在更多下游任务上有更优的泛化能力！

#### 多头注意力机制

Self Attention 就是自注意力机制，假设输入为 $X$，则其计算过程为：
1. 首先将输入通过一层 Linear 映射到 $Q, K, V$，也就是 $Q = X W_Q, K = X W_K, V = X W_V$
2. 然后计算注意力 $\text{Attention}(Q, K, V) = \text{Softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$
这里面类似我们刚才拆分出来向量来演示注意力计算的过程，只不过我们把 $Q, K, V$ 每一个都是多个向量拼接，然后堆叠成一个矩阵。 $QK^T$点积就是计算相似度得分，而这里 $\sqrt{d_k}$是归一化的因子。

这里仍然结合我们刚才说的英语阅读理解的例子，来解释一下 Self Attention 自注意力相当于什么。对于你看到的文本假设为 $X$，而自注意力机制里，$Q, K, V$ 均是来源于文本 $X$。

我们为了理解一篇文章，需要自己设一些思考问题，比如“xx为什么要xx？” 自我提出这些思考问题这一过程也就是 Query 的产生，而对文章信息经过提取后得到的 Key-Value pair，我们要通过带着自己要查询的问题 Query，去和文章每个地方的 Key 计算相似度得分，以判断出现该问题的位置最可能在哪里，然后找到答案 Value 从而理解。这种“自问而自答”，也就很能解释得通为什么被称作为“自注意力”了。

而 Transformer 里采用的这种自注意力机制是**多头**的，也就是 Self Multi-Head Attention，这个多头是什么含义呢？多头就是把整个向量拆分成多块，每块交给一个注意力头计算，然后把各个注意力头计算的结果拼接。

若假设第 $i$ 个注意力头中， $x$对应的那一块先经过 Linear 得到 $Q_i, K_i, V_i$，随后该头计算注意力 $\text{Head}_i = \text{Attention}(Q_i, K_i, V_i) = \text{Softmax}\left(\tfrac{Q_iK_i^T}{\sqrt{d_k}}\right)V_i$，然后把这 $H$ 个头的结果拼接在一起 $\text{Concat}(\text{Head}_1, \cdots, \text{Head}_H)$，最后再经过一个 Linear MLP 的变换 $W_o$，得到最后输出 $\text{MHA}= W_o \text{Concat}(\text{Head}_1, \cdots, \text{Head}_H)$。
- Self Multi-head Attention 多头自注意力机制：主要是把维度 `dim` 拆成 `h` 个头，每个头计算一个注意力机制。就比如，当 `dim=128, h=4`的时候，那也就是 128 维的向量中，1-32维、33-64维、65-96维、97-128维 这四块拆成4个小的向量，每个小向量通过注意力机制得到。
- 这么做有什么好处？多头相比单头的核心思想，正是通过将高维向量拆分为多个子空间，让每个子空间独立学习不同的语义特征，从而避免信息混淆，提升模型的表达能力。
- 仍然我们用上面英语阅读的例子，对于一句话我们大脑编码为 `dim` 维的向量，前面几维数字代表主语、中间几维代表谓宾....我们用多头注意力就是把整个句子拆分为主语、谓语、宾语这几块，避免混在一起，而是分别计算注意力。这样，模型在 Query 为 " Bob 出生于哪一年" 的时候，文章里碰到 "Alice 出生于 1997 "年后面意思对但主语错误时，主语那一部分的 Head 就可以将其区分出来，而不是将其与谓语、宾语等混合在一起，仍然因后面部分的相似而仍有较高的相似度得分！

#### 细节讨论1-- Q, K, V 的来源

在 Encoder 中，每个 Block 里有1个多头自注意力 （Self MHA）层，这个注意力是自注意力的，也即 $Q, K, V$都是来自自身的输入 $X$。

在 Decoder 中，就要分模型的整体架构了
- 如果模型是 Decoder-Only 架构，那每一个 Block 里都有2个 Self MHA 层，同样是自注意力，$Q, K, V$ 同样来自 $X$ 自身的输入。但是为防止模型预先知道未来的信息，每一个 Self MHA 都是单项注意力，即带有 Causal Mask，也即挡掉上三角的部分。
- 如果模型是 Encoder-Decoder Hybird 架构的，那么对于每个 Block 中的2个 MHA，第一个是 Self Attention，也即 $Q, K, V$都来自 $X$ 自身的输入。**而第二个是 Cross Attention!**，也就是 Query 来自 Decoder 中的输入，而 **Key-Value 来自 Encoder 对应那层的输出**！不过，这个和 Encoder 那边输出做交叉注意力 Cross Attention，是双向的，也就是没有 Causal Mask！

在 Decoder里，不接受来自 Encoder 的注意力，也就是仅有 Decoder 自身输入的自注意力中，都有保证单向的 Causal Mask！

#### 细节讨论2-- QKV 计算过程

这里想必是大厂面试里的“重点区！” 每个参加过秋招被问Transformer的时候八成都会碰到这块的问题！

第一个就是 **为什么Q, K之间的相似度用点积计算**？因为是计算相似度，所以首先我们要选择能够反映相似度的计算方式。

- 每个元素都是一个向量，直觉上在一个高维空间里，我们可以通过这两个向量的夹角来判断这两个向量的相似度。这也就是余弦相似度！
- 实际上，另一种可选的方案是 Additive 加法性的，表达式是 $\text{Softmax}(W \cdot \tanh(W_q Q + W_k K))V$。其通过非线性的变换捕捉关系。从直觉上看，我们就发现 Additive 的方法还需要多一个 $W, W_q, W_k$ 这三个参数矩阵，注意不是把 $X$ 嵌入到 $Q, K, V$的那个 $W_q$。也就是说 Additive 参数多，而点积参数少，存储开销小。
- 另外实践上，点积是矩阵乘法，是可以通过硬件加速算法加速计算，而相比之下 Additive 计算就慢一些。

第二个就是 **为什么要除以 $\sqrt{d_k}$ ？**

实际大模型中的 $\sqrt{d_k}$ 是非常大的，也就是嵌入内部后的维度，一般也都 512 维（还算小的）。而这个根号 $d_k$，其实起到了一个归一化作用
- 现在，我们假设 $q, k$两个向量做点积，都是 $d_k$维的。也就是说 $q^T k = \sum_{i=1}^{d_k} q_i k_i$。
- 实际上，我们的网络对于 $q, k$这两个向量中每个元素都是遵循相互独立的同分布。若我们假设每个元素的分布中，均值都为 $\mu$，方差都为 $\sigma^2$。不妨假设 $\mu = 0$。 $q^Tk= \sum_{i=1}^{d_k} q_i k_i$。是 $d_k$ 个同方差的变量之和。我们可以得到 $q_i k_j$的均值是0，而方差是 $\sigma^4$，而相加在一起，就可以得到方差是 $d_k \sigma^4$
- 实践中 $d_k$多非常大，随着维度增大，如果不除以根号 $d_k$，则上面 $q^T k$ 点积的方差 $d_k \sigma^4$就会因为 $d_k$的存在非常大！除以根号 $d_k$，则可以把方差 Scale 归一化到与 $d_k$ 维度无关，就不会有特别大的元素。
- 而上面提到的方差很大，主要的影响会影响到 Softmax 中的计算。Softmax 里面对每个元素进行 $\exp$指数计算。如果出现了很大的值，那 $\exp$这个指数的结果就会异常大！如很可能 $e^{100}$量级，会导致数值溢出！且同时，因为一个值异常的大，Softmax 软最大化会退化为 Argmax！
- 那可不可以例如 $1/2\sqrt{d_k}$这样Scale到更小的值？虽然更不会出现数值溢出，不会退化为 Argmax，但是这样会导致 方差太小，从而Softmax 的软化效果太软，无法有区分度，让整个权值的分布趋于均匀分布！
也就是说，方差太大不行，太小也不行。综合以上几点结合实践， 让方差正好， $\sqrt{d_k}$是最佳的选择。

#### 细节讨论3-- 位置编码

在 Transformer 中 Encoder 和 Decoder的开始，为了赋予数据不同位置序列性关系的数据，会采用位置编码 Positional Encoding的技术。最常用的是正余弦编码，对于 $(\text{pos}, i)$ 位置，也就是一句话中第 $\text{pos}$个单词，嵌入得到向量的第 $i$ 个维度，我们的 Positional Encoding 的公式一般为

$\text{PE}(\text{pos}, 2i) = \sin(\text{pos}/10000^{2i/d_{\text{model}}}) $

$\text{PE}(\text{pos}, 2i+1) = \cos(\text{pos}/10000^{2i/d_{\text{model}}}) $

这里 $d_{\text{model}}$ 就是词嵌入的总维度数。

#### 细节讨论4-- 网络架构

Transformer 里面有 FFN 前馈层，这里面是两层 MLP，中间有一个捕捉非线性的激活函数。一般为 ReLU/ GeLU等。

每次计算 MHA 或者 FFN 后为了防止梯度消失，都有 Residual 残差链接。

第一个：**LayerNorm**，也就是在 MHA 和 FFN 之后，都有一个 LayerNorm 层。这个 LayerNorm 层的作用就是归一化，通过将数据放缩而提升整个训练的稳定性
- 之所以用 LayerNorm（而非 BatchNorm），是因为 LayerNorm 更适合序列性的数据。不同样本（如句子）的序列长度差异较大，BatchNorm 跨句子样本统计对应位置的同一个特征，容易把句子本身的独立性破坏；而 LayerNorm 则对句子本身当成独立样本，对内部的序列长度维度上所有特征归一化，更合适。
- 此外，LayerNorm都是在计算完 Attention / FFN 后通过残差连接完再归一化。这个在计算注意力分数之后算 LayerNorm 的，是 Post-Norm。而另一种设计是在计算 Attention / FFN 前就归一化的 Pre-Norm。
- PreNorm 因 Residual 的完整残差连接，能够保留完整的梯度，更适合深层一些的模型，但是残差路径未归一化，容易导致相邻层的输出相似度较高，正则化能力有限，从而限制模型表达能力。
- PostNorm 则对子层输出和残差路径的联合归一化，能更增强模型鲁棒性，但是残差路径的梯度也会被压缩，在深层易出现梯度爆炸或消失的现象。