# 第二章 大语言模型架构

## Transformer 模型介绍

2017年，谷歌的一篇 《Attention is all you need》提出的Transformer ，以其在处理长序列信息的高性能，在语言模型领域颠覆了传统 RNN、LSTM 等序列模型。要说 Transformer 的很多细节，那可谓是各大大厂面试的 “重中之重”！所以我们来捋一捋，Transformer 你真的了解透了吗？

### 注意力机制

假设我们看一段较长的文字信息，比如：”虽然 xxx，xxx....xxx，.....，但是 yyy“，尽管前面“虽然”这块说了一大堆，但我们都知道这种转折性的文字，主要的信息在 “但是” 之后。因此我们会把我们看文字的注意力集中在后面 “但是” 这一块。

假设我们人眼识别一张图片中的物体，那么我们会把我们更多的注意力放在这个物体上，而相比之下物体所处的背景没有那么重要，也就自然不会调动更多的注意力去注意之。

Transformer 中的注意力模块，其设计的灵感也是来源于此！
