# Course 1: Flow and Diffusion Models

我们都知道，从 Stable Diffusion 到 Sora，Diffusion 在 AIGC 中一次又一次推动着生成式 AI 的快速发展。 在图像生成、AI 绘画、创作......等很多领域， Diffusion 都占据了非常主导的地位。

从 Diffusion 到 Flow Matching....你真的了解 Diffusion Model 了吗？

## 什么是“采样”生成数据？

我们都知道生成，可在 AI 里什么叫生成？这个应该如何用计算机去形式化定义？

假设有一张图片，在计算机中一般储存为 `[H, W, C]`的形式，视频是多张图片帧的拼接，也就是 `[T, H, W, C]`的形式。现在让你“生成一张猫的图片”—— 你大脑是如何思考，然后在纸上画出一只猫的？

我们对图片是什么的理解，其实抽象起来理解，是一种“概率分布”的形式！比如一张猫的图片，那我们就会定义一个“猫图片”的分布....如何判断生成的质量好？那就是要看生成的图片和我们脑海中“猫图片分布”的匹配度。条件生成：自然就是给定一个label $y$，一般为图像的类别，或是语言的提示词 等等。

对于生成数据，本质抽象理解应该是：**拟合出数据的概率分布，然后从拟合的分布中采样**！这也就是 Generation as Sampling 的主要思想！ $z\sim p_{\text{data}}$ 。而我们要做的，就是用一个模型，学习这个 $p_{\text{data}}$。假设数据集是包含数百万张图片的 Imagenet，那我们就是要通过这几百万个图片，去学习 Imagenet 底层的分布。对于深度学习时代，基本都是把这个拟合的分布用参数化的网络表示学习。

- 判别机器学习一般侧重学习  $p(y|x)$ 条件分布，而生成机器学习则侧重学习  $p(x, y)$ 联合分布
- 生成的目标对象，一般都通过 vector 表征，比如 图像的 `[H, W, C]`，视频的 `[T, H, W, C]`  等，都是以一种向量 $\mathbb{R}^d$ 的形式存储并表征。
- 生成式 AIGC 通过对数据集 $\{z_i}_{i=1}^N$ 学习该数据集底层服从的抽象的数据分布 $z_i \sim p_{\text{data}}^\theta$。
- 条件生成，就是对于数据 $\{(x_i, y_i)}_{i=1}^N$，我们对于给定条件 $y$，学习拟合服从条件的条件分布 $p_{\text{data}}(\cdot | y)$

以上就是我们对生成模型原理和底层表达形式逻辑的抽象化理解。在我们的 Diffusion & Flow Matching 中，本质上就是将一个易于采样的分布（如：高斯分布），通过模型变换到数据的分布。

可以想象：假设有很多鹅卵石雕刻的石雕，鹅卵石在海边特别多，是容易采样的分布；我们要从一个初始的鹅卵石出发，学习鹅卵石从圆润光滑到变成精美石雕塑的过程。很多艺术创作的本质，底层讲也是一种生成模型。

## Flow & Diffusion Model

我们将生成模型的任务定义为从拟合的数据分布中采样。这一过程可以通过将易于采样的分布，变换到数据的分布。对于数据的分布 $p_{\text{data}}$，我们可以从一个初始化的分布 $p_{\text{init}}$ （例如：标准高斯分布 $\mathcal N(0, I)$）开始，通过一系列的变换，逐步变换预测 $p_{\text{data}}$。下面我们从 Flow 的定义开始，结合微分方程，描述一下这一过程可以如何通过模拟得到。包括 ODE、SDE，我们将描述如何定义这个 ODE、SDE 以及如何用网络去将这一过程的网络参数化。

### Flow Model

我们先介绍一下 Flow：我们考虑数据在 $\mathbb{R}^d$ 这一个大的空间之中。我们定义一个ODE常微分方程：

$X: [0, 1] \to \mathbb{R}^d, \quad t \mapsto X_t$。

这个微分方程的解是一个轨迹，也就是数据 $X$ 从 $t=0$ 时刻到 $t=1$ 时刻的变化过程。

我们上面给出了时间的维度$t$，对于整个数据空间也有一个维度。对于每个数据都可以有一个这样的 ODE，于是整体我们可以用一个 vector field 向量场表示，也就是可以定义为

$u: \mathbb{R}^d \times [0, 1]\to \mathbb{R}^d, \quad (x, t)\mapsto u_t(x)$

对于每个时间 $t$ 和空间内的数据 $x$，我们可以得到一个 vector $u_t(x)$，表示数据 $x$ 在时间 $t$ 时刻的变化方向。这其实就是给定时间和空间的特定点，在这一点给出一个 "velocity" 速度。这样，对于数据 $X_t$，也就是在 $t$ 时刻的 $X_t$，我们让其变化的速度方向是 $u_t(X_t)$。并且假设已知了初始时刻 $X_0 = x_0$。于是我们的 ODE 可以形式化为

$\begin{cases} \dfrac{\mathrm d X_t}{\mathrm d t} & = u_t(X_t) \\ X_0 & = x_0 \end{cases}$

也就是说，我们让一个数据从 $x_0$ 出发，如果用 $X_t = X(t)$ 表示这个数据的轨迹过程，那么这个轨迹在时间维度上就是遵循上面的微分方程。现在问题来了。给定数据从 $x_0$ 出发，这个数据的轨迹记为  $X_t$，初值也即 $X_0 = x_0$，那在中间的任意一个时刻 $X_t$ 我们已经形式化定义了，如何求解出之呢？**这其实就是一个Flow流函数**，其也正是上面 ODE 的解。

也就是说，上面形式化的 ODE 问题，其解就是流函数。也即

$\psi: \mathbb{R}^d\times [0, 1]\to \mathbb{R}^d , \quad (x_0, t) \mapsto \psi_t(x_0)$。其满足上面定义的 ODE，也就是

$\begin{cases} \dfrac{\mathrm d \psi_t(x_0)}{\mathrm d t} & = u_t(\psi_t(x_0)) \\ \psi_0(x_0) & = x_0 \end{cases}$

这样，对于给定的初值 $X_0 = x_0$，可以得到其中间一刻的点。$X_t = \psi_t(x_0)$。注意这里的大 $X$ 其实是表达成了一个函数，而 $x_0$ 这个小 $x$ 是特定的点元素。于是我们有如下关系

- 向量场一般是一个速度场，其输入一个点（包括时空信息 $x, t$ ）返回整个场中该点的变化趋势，也就是“速度”。
- 整个向量场是一个速度场，其可以定义一个 ODE 常微分方程
- 这个由向量场定义的 ODE 的解就是 Flows 流

初看这个的人可能会把这些记号搞混，现在我举一个例子把这些的关系澄清一下。

- $X_t$ 是一个函数，表示一个轨迹。初看的人可能会把这个 $X_t$ 混淆当做一个特定的点。但其实你完全可以把 $X_t$ 写成 $X(t)$，这样就不大会搞混
- 假设有一个静电打印机，这个打印机的油墨最初呈现一个 cluster，像高斯分布。现在你要打出一个形状（例如 Swiss roll），那么从墨盒到纸张通过增加一个电场，让每个位置的带电油墨粒子通过电场，走一个轨迹。这个电场给予了带电的油墨粒子在空隙（纸张和墨盒之间）每一个点的速度。纸的平面 xy 你可以当做空间维度，而间隙这一块可以当做时间维度（在墨盒上视为 $t=0$，而到达纸张时 $t=1$）。这个过程中，通过增加的电场，让带电的油墨粒子沿着轨迹走。这个轨迹就是 Flow 流。
- 这样，$x_0$ 就表示了———带电油墨粒子初始的空间位置，整个空间加的电场即 $u(t, x) = u_t(x)$ 表示一阶导数。给定了整个电场空间和出发点 $x_0$，现在想求这个带电粒子的轨迹过程，也就是 $X(t)$，满足 $X(0)=x_0$，而这个 $X(t)$ 可以通过求解微分方程得到，这个解为 $\psi(t, x_0) = \psi_t(x_0)$。其表示了从$x_0$出发，在中间任意时刻 $t$ 时点的位置。
- $X(t)$ 一般只定义 $t$，其是特定一个粒子 （ $x_0$ 出发的那个）在任意时刻的轨迹。
- $\psi(t, x_0)$，也就是 $\psi_t(x_0)$，就是求解出来的解，给出从 $x_0$ 出发的粒子在中间 $t$ 时刻到达的位置。

数学上，如果 $u$ 是连续可微，且微分有界（Lipschitz连续了）的话，那么结合 ODE 解的存在唯一性定理，上面的 ODE 给出的解 $\psi_t$ 是唯一的。实际上，这个 $\psi_t$ 在我们这里是一个微分同胚。（刚在黎曼几何听完后hhh）

**当然，这里面的速度场如果形式比较复杂，我们就很难求出上面 ODE 的解析解**。这时我们可以用数值解去近似。最简单的一阶欧拉法也即：对于 $X_0=X(0)=x_0$ 初值，可以一阶欧拉迭代近似得到 $X_1 = X(1)$
$ X(t+h) = X(t) + h u(t, X(t)), \quad t = 0, h, 2h, \cdots, 1-h$
或者是通过 Heun 方法。但总之是通过数值解去逼近。

**上面我们给出了流的定义以及如何形式化这个问题**。现在我们考虑整个 Flow 是一个生成模型。前面提到，我们这里的生成模型本质是将一个易于采样的分布变换到另一个数据上的分布。这个 Flow model 可以于是定义为

$\begin{cases} \dfrac{\mathrm d X_t}{\mathrm d t}  = u_t^\theta(X_t) \\ X_0 \sim p_{\text{init}} \end{cases}$

其中我们用一个神经网络将这个 vector field 速度场 $u(t, x) = u_t(x)$ 参数化为 $u^theta(t, x) = u_t^\theta(x)$。我们的目标就是希望通过这一 ODE 求出的轨迹在 $t=1$ 终值时符合 $p_{\text{data}}$ 数据的分布。也就是对于其解 $\psi_t^\theta(x_0)$ 在 $t=1$时

$X_1 \sim p_{\text{data}}\quad \Leftrightarrow\quad \psi^\theta(1, x_0) = \psi_{1}^\theta(x_0) \sim p_{\text{data}}$

其中 $\psi_t^\theta(x_0)$ 表示向量场 $u_t^\theta(x)$ 诱导出的出发于 $x_0$ 的轨迹，也就是 Flow。

### Diffusion Model

上面我们给出的是将问题形式化为 ODE 的形式，主要是 Flow Matching 的基础，而 Diffusion 本质上是 SDE 随机微分方程，是 Score Matching！

而对于 Diffusion Model，其过程可以表示为一个随机微分方程 SDE！轨迹也一般都是波动，具有一定随机性的！这一般是一个随机过程，也就是：

$X: [0, 1] \to \mathbb{R}^d,\quad t\mapsto X_t$ 对于每个 $X$ 是一个随机的轨迹！ SDE 通过一个布朗运动建立，这个过程 $W = (W_t)_{0\leq t \leq 1}$ 满足： $W_0 = 0$，以及

- 正态分布的可加性： $W_t - W_s \sim \mathcal N (0, (t-s)I)$，每次过一个时间步，就相当于概率密度函数中的正态分布方差增加。
- 可加关系是独立的：也就是相邻两次的递增 --- $W_1 - W_0$、 $W_2 - W_1$ 等，都是独立的随机变量

这个布朗运动也叫维纳过程，我们也可以如下表示：对于步长 $h>0$，我们设置 $W_0 = 0$，有

$W_{t+h} = W_t + \sqrt{h}\epsilon_t\quad \epsilon_t \sim \mathcal N (0, I)$，这样可使得经过时间步 $h$ 后正态分布的方差增大 $h$。

**注意，因为从 ODE 转变为 SDE 了，整个过程变得随机化了，因此我们不能再使用导数微分！** 我们把原来的 ODE 可以改写为

$\dfrac{\mathrm d X_t}{\mathrm d t} = u_t(X_t) \Leftrightarrow X_{t+h} - X_t = h u_t(X_t) + hR_t(h) $

这里面当 $h$ 足够小时最右边的 $R(h)$ 误差可以忽略不计。现在我们要像这个 ODE 中引入一些随机的过程项，使之变为 SDE：

$X_{t+h} = X_t + h u_t(X_t) +  \sigma_t(W_{t+h}-W_t) + hR_t(h)$

这里面 SDE 里，$u_t(X_t)$ 是 drift coefficient，而 $\sigma_t$ 是 diffusion coefficient。这个 SDE 我们可以写为

$\mathrm d X_t = u_t(X_t) \mathrm d t + \sigma_t \mathrm d W_t; \quad X_0 = x_0$

这也就是类似可以如下定义 Diffusion Model：

$\begin{cases} \mathrm d X_t  = u_t^\theta(X_t) \mathrm d t+ \sigma_t \mathrm d W_t \\ X_0 \sim p_{\text{init}} \end{cases}$

类比在 ODE 中的 $X_{t+h} = X_t + h u_t^\theta (X_t)$ 的一阶 Euler 法，我们可以类似定义 $X_0 = x_0$ 用 Euler-Maruyama 法得到 SDE 的类似数值采样过程为

$X_{t+h} = X_t + h u_t^\theta (X_t) + \sqrt{h}\sigma_t \epsilon_t, \quad \epsilon_t \sim \mathcal N(0, I)$

同样，我们的目标是希望到 $t=1$ 时还原回数据中的分布 $X_1 \sim p_{\text{data}}$
