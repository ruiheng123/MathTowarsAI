## Unified Video-Action Model

[Paper 传送门](https://arxiv.org/pdf/2503.00200v1)

当 Video 视频模型和 Action 动作这二者统一在一起时，会有什么效果？

我们都知道在学习一项技能时，比如要安装一个衣架，如果身边没有安装师傅，而运到的衣架是散的，他一般会给一个安装视频。

视频因为时间一致性、连续性，能够反馈一段连续的信息，相比之下图片则提供的信息就偏少。

像之前字节的 GR-2，其是将一个 Foundation model 在大规模的视频数据上进行训练，然后再在下游的机器人任务里微调。而且其输出既包含了动作信息，也同时在预测动作的下一帧。

而这一篇paper 是 Shuran Song 出品！视频和动作统一了这么多方面，单看一眼图片——太有意思了！
![1741158438447](image/UVA/1741158438447.png)

视频往往包含大量的信息，但处理起来是一个不小的开销。视频既包含时间也包含空间，是 Spatial + Temporal 的结合，而 Spatial 信息在视频中每一帧 [H, W] 的图片就已经有很大的存储。而对于动作的生成，为了让 Action 面向机器人，我们又需要保证执行动作的实时性。但视频的信息处理也会消耗很多时间。

仅预测动作，缺少了视频的监督，虽然速度提了上来，但是生成的动作序列也难以执行成功任务。而加入了视频理解的监督，在以往工作之中也出现了很多降低推理速度的问题，此外从视觉空间到动作物理空间的gap也导致了一定误差。

作者是如何平衡在的？答案是：Video 和 Action Unify 在一起！作者把 Video 和 Action 的统一，让 latent 表征同时包含视觉和动作的信息！

- 视觉与动作表征统一：同时从视频与动作学习，视频和动作一起监督。这样既可以理解视频并生成后续，还可以根据执行动作理解动力学！
- 生成视频和生成动作的 Diffusion Head 分离解耦。
