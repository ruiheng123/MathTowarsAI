{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masked Autoencoders\n",
    "\n",
    "MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from einops import repeat\n",
    "\n",
    "from vit_pytorch.vit import Transformer\n",
    "\n",
    "class MAE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        encoder,\n",
    "        decoder_dim,\n",
    "        masking_ratio = 0.75,\n",
    "        decoder_depth = 1,\n",
    "        decoder_heads = 8,\n",
    "        decoder_dim_head = 64\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert masking_ratio > 0 and masking_ratio < 1, 'masking ratio must be kept between 0 and 1'\n",
    "        self.masking_ratio = masking_ratio\n",
    "\n",
    "        # extract some hyperparameters and functions from encoder (vision transformer to be trained)\n",
    "\n",
    "        self.encoder = encoder\n",
    "        num_patches, encoder_dim = encoder.pos_embedding.shape[-2:]\n",
    "\n",
    "        self.to_patch = encoder.to_patch_embedding[0]\n",
    "        self.patch_to_emb = nn.Sequential(*encoder.to_patch_embedding[1:])\n",
    "\n",
    "        pixel_values_per_patch = encoder.to_patch_embedding[2].weight.shape[-1]\n",
    "\n",
    "        # decoder parameters\n",
    "        self.decoder_dim = decoder_dim\n",
    "        self.enc_to_dec = nn.Linear(encoder_dim, decoder_dim) if encoder_dim != decoder_dim else nn.Identity()\n",
    "        self.mask_token = nn.Parameter(torch.randn(decoder_dim))\n",
    "        self.decoder = Transformer(dim = decoder_dim, depth = decoder_depth, heads = decoder_heads, dim_head = decoder_dim_head, mlp_dim = decoder_dim * 4)\n",
    "        self.decoder_pos_emb = nn.Embedding(num_patches, decoder_dim)\n",
    "        self.to_pixels = nn.Linear(decoder_dim, pixel_values_per_patch)\n",
    "\n",
    "    def forward(self, img):\n",
    "        device = img.device\n",
    "\n",
    "        # get patches\n",
    "\n",
    "        patches = self.to_patch(img)\n",
    "        batch, num_patches, *_ = patches.shape\n",
    "\n",
    "        # patch to encoder tokens and add positions\n",
    "\n",
    "        tokens = self.patch_to_emb(patches)\n",
    "        if self.encoder.pool == \"cls\":\n",
    "            tokens += self.encoder.pos_embedding[:, 1:(num_patches + 1)]\n",
    "        elif self.encoder.pool == \"mean\":\n",
    "            tokens += self.encoder.pos_embedding.to(device, dtype=tokens.dtype) \n",
    "\n",
    "        # calculate of patches needed to be masked, and get random indices, dividing it up for mask vs unmasked\n",
    "\n",
    "        num_masked = int(self.masking_ratio * num_patches)\n",
    "        rand_indices = torch.rand(batch, num_patches, device = device).argsort(dim = -1)\n",
    "        masked_indices, unmasked_indices = rand_indices[:, :num_masked], rand_indices[:, num_masked:]\n",
    "\n",
    "        # get the unmasked tokens to be encoded\n",
    "\n",
    "        batch_range = torch.arange(batch, device = device)[:, None]\n",
    "        tokens = tokens[batch_range, unmasked_indices]\n",
    "\n",
    "        # get the patches to be masked for the final reconstruction loss\n",
    "\n",
    "        masked_patches = patches[batch_range, masked_indices]\n",
    "\n",
    "        # attend with vision transformer\n",
    "\n",
    "        encoded_tokens = self.encoder.transformer(tokens)\n",
    "\n",
    "        # project encoder to decoder dimensions, if they are not equal - the paper says you can get away with a smaller dimension for decoder\n",
    "\n",
    "        decoder_tokens = self.enc_to_dec(encoded_tokens)\n",
    "\n",
    "        # reapply decoder position embedding to unmasked tokens\n",
    "\n",
    "        unmasked_decoder_tokens = decoder_tokens + self.decoder_pos_emb(unmasked_indices)\n",
    "\n",
    "        # repeat mask tokens for number of masked, and add the positions using the masked indices derived above\n",
    "\n",
    "        mask_tokens = repeat(self.mask_token, 'd -> b n d', b = batch, n = num_masked)\n",
    "        mask_tokens = mask_tokens + self.decoder_pos_emb(masked_indices)\n",
    "\n",
    "        # concat the masked tokens to the decoder tokens and attend with decoder\n",
    "        \n",
    "        decoder_tokens = torch.zeros(batch, num_patches, self.decoder_dim, device=device)\n",
    "        decoder_tokens[batch_range, unmasked_indices] = unmasked_decoder_tokens\n",
    "        decoder_tokens[batch_range, masked_indices] = mask_tokens\n",
    "        decoded_tokens = self.decoder(decoder_tokens)\n",
    "\n",
    "        # splice out the mask tokens and project to pixel values\n",
    "\n",
    "        mask_tokens = decoded_tokens[batch_range, masked_indices]\n",
    "        pred_pixel_values = self.to_pixels(mask_tokens)\n",
    "\n",
    "        # calculate reconstruction loss\n",
    "\n",
    "        recon_loss = F.mse_loss(pred_pixel_values, masked_patches)\n",
    "        return recon_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11, 12, 13, 14, 15],\n",
      "        [16, 17, 18, 19, 20, 21, 22, 23],\n",
      "        [24, 25, 26, 27, 28, 29, 30, 31],\n",
      "        [32, 33, 34, 35, 36, 37, 38, 39],\n",
      "        [40, 41, 42, 43, 44, 45, 46, 47],\n",
      "        [48, 49, 50, 51, 52, 53, 54, 55],\n",
      "        [56, 57, 58, 59, 60, 61, 62, 63]])\n",
      "tensor([[[[ 0,  1,  2,  3],\n",
      "          [ 4,  5,  6,  7]],\n",
      "\n",
      "         [[ 8,  9, 10, 11],\n",
      "          [12, 13, 14, 15]],\n",
      "\n",
      "         [[16, 17, 18, 19],\n",
      "          [20, 21, 22, 23]],\n",
      "\n",
      "         [[24, 25, 26, 27],\n",
      "          [28, 29, 30, 31]]],\n",
      "\n",
      "\n",
      "        [[[32, 33, 34, 35],\n",
      "          [36, 37, 38, 39]],\n",
      "\n",
      "         [[40, 41, 42, 43],\n",
      "          [44, 45, 46, 47]],\n",
      "\n",
      "         [[48, 49, 50, 51],\n",
      "          [52, 53, 54, 55]],\n",
      "\n",
      "         [[56, 57, 58, 59],\n",
      "          [60, 61, 62, 63]]]])\n",
      "tensor([[[[ 0,  1,  2,  3],\n",
      "          [ 8,  9, 10, 11],\n",
      "          [16, 17, 18, 19],\n",
      "          [24, 25, 26, 27]],\n",
      "\n",
      "         [[ 4,  5,  6,  7],\n",
      "          [12, 13, 14, 15],\n",
      "          [20, 21, 22, 23],\n",
      "          [28, 29, 30, 31]]],\n",
      "\n",
      "\n",
      "        [[[32, 33, 34, 35],\n",
      "          [40, 41, 42, 43],\n",
      "          [48, 49, 50, 51],\n",
      "          [56, 57, 58, 59]],\n",
      "\n",
      "         [[36, 37, 38, 39],\n",
      "          [44, 45, 46, 47],\n",
      "          [52, 53, 54, 55],\n",
      "          [60, 61, 62, 63]]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 8,  9, 10, 11],\n",
       "         [16, 17, 18, 19],\n",
       "         [24, 25, 26, 27]],\n",
       "\n",
       "        [[ 4,  5,  6,  7],\n",
       "         [12, 13, 14, 15],\n",
       "         [20, 21, 22, 23],\n",
       "         [28, 29, 30, 31]],\n",
       "\n",
       "        [[32, 33, 34, 35],\n",
       "         [40, 41, 42, 43],\n",
       "         [48, 49, 50, 51],\n",
       "         [56, 57, 58, 59]],\n",
       "\n",
       "        [[36, 37, 38, 39],\n",
       "         [44, 45, 46, 47],\n",
       "         [52, 53, 54, 55],\n",
       "         [60, 61, 62, 63]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(64).reshape(8, 8)\n",
    "print(A)\n",
    "print(A.reshape(2, 4, 2, 4))\n",
    "print(A.reshape(2, 4, 2, 4).permute(0, 2, 1, 3))\n",
    "A.reshape(2, 4, 2, 4).permute(0, 2, 1, 3).reshape(4, 4, 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
